@Article{GlmNet,
    title = {Regularization Paths for Generalized Linear Models via
      Coordinate Descent},
    author = {Jerome Friedman and Trevor Hastie and Robert Tibshirani},
    journal = {Journal of Statistical Software},
    year = {2010},
    volume = {33},
    number = {1},
    pages = {1--22},
    url = {https://www.jstatsoft.org/v33/i01/},
  }

@article{CompetitionSource,
author = {Putten, Peter and Ruiter, Michel and Someren, Maarten},
year = {2000},
month = {01},
pages = {},
title = {CoIL Challenge 2000 Tasks and Results: Predicting and Explaining Caravan Policy Ownership}
}

@article{CompWinner,
author = {Elkan, Charles},
year = {2001},
month = {07},
pages = {},
title = {Magical Thinking in Data Mining: Lessons From CoIL Challenge 2000},
journal = {Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
doi = {10.1145/502512.502576}
}

@article{sampling,
author = {Batista, Gustavo E. A. P. A. and Prati, Ronaldo C. and Monard, Maria Carolina},
title = {A Study of the Behavior of Several Methods for Balancing Machine Learning Training Data},
year = {2004},
issue_date = {June 2004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {1},
issn = {1931-0145},
url = {https://doi.org/10.1145/1007730.1007735},
doi = {10.1145/1007730.1007735},
abstract = {There are several aspects that might influence the performance achieved by existing learning systems. It has been reported that one of these aspects is related to class imbalance in which examples in training data belonging to one class heavily outnumber the examples in the other class. In this situation, which is found in real world data describing an infrequent but important event, the learning system may have difficulties to learn the concept related to the minority class. In this work we perform a broad experimental evaluation involving ten methods, three of them proposed by the authors, to deal with the class imbalance problem in thirteen UCI data sets. Our experiments provide evidence that class imbalance does not systematically hinder the performance of learning systems. In fact, the problem seems to be related to learning with too few minority class examples in the presence of other complicating factors, such as class overlapping. Two of our proposed methods deal with these conditions directly, allying a known over-sampling method with data cleaning methods in order to produce better-defined class clusters. Our comparative experiments show that, in general, over-sampling methods provide more accurate results than under-sampling methods considering the area under the ROC curve (AUC). This result seems to contradict results previously published in the literature. Two of our proposed methods, Smote + Tomek and Smote + ENN, presented very good results for data sets with a small number of positive examples. Moreover, Random over-sampling, a very simple over-sampling method, is very competitive to more complex over-sampling methods. Since the over-sampling methods provided very good performance results, we also measured the syntactic complexity of the decision trees induced from over-sampled data. Our results show that these trees are usually more complex then the ones induced from original data. Random over-sampling usually produced the smallest increase in the mean number of induced rules and Smote + ENN the smallest increase in the mean number of conditions per rule, when compared among the investigated over-sampling methods.},
journal = {SIGKDD Explor. Newsl.},
month = {jun},
pages = {20–29},
numpages = {10}
}

@article{ElasticNet,
author = {Zou, Hui and Hastie, Trevor},
title = {Regularization and variable selection via the elastic net},
journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
volume = {67},
number = {2},
pages = {301-320},
keywords = {Grouping effect, LARS algorithm, Lasso, Penalization, p≫n problem, Variable selection},
doi = {https://doi.org/10.1111/j.1467-9868.2005.00503.x},
url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9868.2005.00503.x},
eprint = {https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-9868.2005.00503.x},
abstract = {Summary.  We propose the elastic net, a new regularization and variable selection method. Real world data and a simulation study show that the elastic net often outperforms the lasso, while enjoying a similar sparsity of representation. In addition, the elastic net encourages a grouping effect, where strongly correlated predictors tend to be in or out of the model together. The elastic net is particularly useful when the number of predictors (p) is much bigger than the number of observations (n). By contrast, the lasso is not a very satisfactory variable selection method in the p≫n case. An algorithm called LARS-EN is proposed for computing elastic net regularization paths efficiently, much like algorithm LARS does for the lasso.},
year = {2005}
}

@online{Discretization,
  author = {Gupta, Rohan},
  title = {An Introduction to Discretization Techniques for Data Scientists},
  year = {2019},
  month = {dec}, 
  day = {06},
  url = {https://towardsdatascience.com/an-introduction-to-discretization-in-data-science-55ef8c9775a2},
  urldate = {2022-02-26}
}

@article{BiasVar,
  title={A bias-variance analysis of a real world learning problem: The CoIL challenge 2000},
  author={Van Der Putten, Peter and Van Someren, Maarten},
  journal={Machine learning},
  volume={57},
  number={1},
  pages={177--195},
  year={2004},
  publisher={Springer}
}

@article{KimStreet,
  title={CoIL challenge 2000: Choosing and explaining likely caravan insurance customers},
  author={Kim, Y and Street, WN},
  journal={Technical Report 2000-09},
  year={2000}
}